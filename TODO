Bowtie 2 TODO list:
- Restore --overhang flag
- Soft/hard trimming taken into account when setting up paired-end problems?
- Bunch up sensitivity arguments into neat packages
- More sophisticated MAPQ calculation
  + Taking paired-end alignment into account
  + More local-alignment savvy
  + Simulate across various read lengths

Manuscript TODO:
- Get SOAP2 benchmarks working

Performance ideas:
- Do some of the gather & backtrace steps in SSE code
- Add resolved offsets to cache
- Add faster DP filter to run first
- Binary search to find reference records instead of linear scan
- Align seeds in random order and investigate each seed immediately after
  aligning it
- Let opposite mate finding be driven by seed hits, not spread across a huge
  window
- Have a cache where we've precalculated all the range info for low-complexity
  sequences like AA, AC, AG, AT, etc. repeated (only 16 of those)

How to compare tools:

- Speed/sensitivity
- MAPQ failures
- A aligned / B failed to align
- Alignment length? (average? MAPQ-like comparison?)

Mapping quality evaluation ideas:

Our "accuracy" argument in the Bowtie 2 paper will boil down to an argument
about why the mapping qualities reported are reasonable.  So how do we assess
mapping quality?  We can get at the "truth" in various ways:

1. Simulate, stratify, count.  Simulate reads and keep a record of where they're
   simulated from.  Align them, then stratify alignments by MAPQ.  For each
   stratum, check whether the stratum MAPQ fits the actual fraction of the
   alignments for which the read's location is determined correctly.
   
   Pros:
    - It's simple
    - Works equally well for paired, unpaired data
   Cons:
    - It may be hard to get a representative samples across all possible MAPQs.
      I.e. if we want to get a good estimate all the way up through MAPQ 40,
      which equals a 1 in 10,000 chance of having the wrong alignment, we
	  probably need millions...
   Questions:
    - Can we use this as a starting point to gain confidence in a model-based
	  method for getting "gold standard" MAPQs?

2. Use "gold standard" aligner & a demonstrably accurate MAPQ calculated based
   on the space of all alignments; not just those that pass through the seed
   filter and rise above a threshold.  The problem is: we ultimately have to
   calcualte MAPQs from alignment scores that in turn come from a model, a
   model that might suck.  How much does this mess things up?  Well, we can
   evaluate the gold standard using 1.

3. Simulate, get "gold standard" MAPQs from model.  If we simulate a read from
   a subject genome, then align the read back to the subject genome with a very
   sensitive aligner, we should be able to calculate a good mapping quality by
   summing over Phred penalties for all the mismatches.  

4. Hi/lo.  If one tool reports a high mapping quality for an alignment (say,
   > 30) and another tool reports a low mapping quality (say, <= 5), then the
   tool that reported the low mapping quality has usually found a good
   second-best alignment and is usually the more accurate of the two.
   
Simulation:

How sophisticated does this have to be?  We can avoid the complication of
having to simulate variants by using another already-assembled genome.  E.g.
for human, we can use one of the Complete Genomics genomes.  Maybe we could do
something similar for Arabidopsis.

We do have to simulate sequencing error and other phenomena somehow.  For
unpaired Illumina data, we just have to simulate errors, which we can do by
borrowing quality strings from real data and then introducing error according
to the meaning of the quality values.  For 454 data, we have to do something
similar, but using a homopolymer model.  For paired-end data, there's the extra
complication of having to simulate fragment lengths, but that can also be
borrowed from real data.
